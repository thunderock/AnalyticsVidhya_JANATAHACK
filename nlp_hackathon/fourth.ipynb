{
"cells": [
{
"metadata": {
"_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
"_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
"trusted": true
},
"cell_type": "code",
"source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
"execution_count": 1,
"outputs": [
{
"output_type": "stream",
"text": "/kaggle/input/hacklive-3-guided-hackathon-nlp/Test.csv\n/kaggle/input/hacklive-3-guided-hackathon-nlp/SampleSubmission.csv\n/kaggle/input/hacklive-3-guided-hackathon-nlp/Tags.csv\n/kaggle/input/hacklive-3-guided-hackathon-nlp/Train.csv\n",
"name": "stdout"
}
]
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "# !wget https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/tensorflow_models/scibert_scivocab_uncased.tar.gz\n",
"execution_count": 2,
"outputs": []
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "# # !mkdir scibert_scivocab_uncased\n# !tar -xvf ./scibert_scivocab_uncased.tar.gz\n# ! mv scibert_scivocab_uncased/bert_config.json scibert_scivocab_uncased/config.json\n# ! mv scibert_scivocab_uncased/bert_model.ckpt.data-00000-of-00001 scibert_scivocab_uncased/tf_model.h5\n# !ls scibert_scivocab_uncased\n",
"execution_count": 3,
"outputs": []
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "!pip install -q transformers==2.11.0\n!wget https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/tensorflow_models/scibert_scivocab_uncased.tar.gz\n!tar -xvf ./scibert_scivocab_uncased.tar.gz\nos.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning\n!transformers-cli convert --model_type bert \\\n  --tf_checkpoint './scibert_scivocab_uncased/bert_model.ckpt' \\\n  --config './scibert_scivocab_uncased/bert_config.json' \\\n  --pytorch_dump_output './scibert_scivocab_uncased/pytorch_model.bin'",
"execution_count": 4,
"outputs": [
{
"output_type": "stream",
"text": "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n\nWe recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n\nallennlp 1.1.0 requires transformers<3.1,>=3.0, but you'll have transformers 2.11.0 which is incompatible.\u001b[0m\n--2020-10-23 12:34:38--  https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/tensorflow_models/scibert_scivocab_uncased.tar.gz\nResolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.181.208\nConnecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.181.208|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1216161420 (1.1G) [application/x-tar]\nSaving to: ‘scibert_scivocab_uncased.tar.gz’\n\nscibert_scivocab_un 100%[===================>]   1.13G  37.8MB/s    in 29s     \n\n2020-10-23 12:35:08 (39.7 MB/s) - ‘scibert_scivocab_uncased.tar.gz’ saved [1216161420/1216161420]\n\nscibert_scivocab_uncased/\nscibert_scivocab_uncased/bert_model.ckpt.data-00000-of-00001\nscibert_scivocab_uncased/bert_model.ckpt.index\nscibert_scivocab_uncased/vocab.txt\nscibert_scivocab_uncased/bert_model.ckpt.meta\nscibert_scivocab_uncased/bert_config.json\n2020-10-23 12:35:33.983971: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\nBuilding PyTorch model from configuration: BertConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 31090\n}\n\nSave PyTorch model to ./scibert_scivocab_uncased/pytorch_model.bin\n",
"name": "stdout"
}
]
},
{
"metadata": {
"_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
"_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
"trusted": true
},
"cell_type": "code",
"source": "\nimport tensorflow as tf\nimport transformers\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import backend as K\nimport pickle as pkl\nimport gc\nimport logging\nimport warnings\nimport time\nimport sys",
"execution_count": 5,
"outputs": []
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "bert_model_name = './scibert_scivocab_uncased'\n\nconfig = transformers.BertConfig.from_json_file('./scibert_scivocab_uncased/bert_config.json')\n\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(level=logging.ERROR)\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\ntf.config.experimental.set_memory_growth(gpus[0], True)\n",
"execution_count": 6,
"outputs": []
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "\nmax_length = 512\nbatch_size = 32\nepochs = 5\ndf = pd.read_csv('/kaggle/input/hacklive-3-guided-hackathon-nlp/Train.csv')\n\nX = df.iloc[:, 1: 6]\ny = df.iloc[:, 6: 6 + 25]\n\nLABELS = y.columns\n\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, shuffle=True, test_size=.2)\ntrain_X.shape, train_y.shape, test_X.shape, test_y.shape\n\n\ndel train_X, train_y\n\n",
"execution_count": 7,
"outputs": []
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "\n\nX = X.reset_index(drop=True)\ny = y.reset_index(drop=True)\ntest_y = test_y.reset_index(drop=True)\ntest_X = test_X.reset_index(drop=True)\n\n\ntest_y = test_y.values\ny = y.values\n\n",
"execution_count": 8,
"outputs": []
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "\n\nclass BertSemanticDataGenerator(tf.keras.utils.Sequence):\n\n    def __init__(\n        self,\n        sentences,\n        depts,\n        labels,\n        batch_size=batch_size,\n        shuffle=True,\n        include_targets=True,\n    ):\n        self.sentences = sentences\n        self.depts = depts\n        self.labels = labels\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.include_targets = include_targets\n#         self.tokenizer = transformers.BertTokenizer.from_pretrained(\n#             \"bert-base-uncased\", do_lower_case=True, max_length=2048\n#             )\n        self.tokenizer = transformers.BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\n        self.indexes = np.arange(len(self.sentences))\n        self.on_epoch_end()\n\n    def __len__(self):\n        return len(self.sentences) // self.batch_size\n\n    def __getitem__(self, idx):\n        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n        sentences = self.sentences[indexes]\n\n        encoded = self.tokenizer.batch_encode_plus(\n            sentences.tolist(),\n            add_special_tokens=True,\n            max_length=max_length,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n            pad_to_max_length=True,\n            return_tensors=\"tf\",\n        )\n\n\n        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n\n        dept_ids = np.array(self.depts[indexes], dtype=\"int32\")\n        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n\n        if self.include_targets:\n            labels = np.array(self.labels[indexes], dtype=\"int32\")\n            return [input_ids, dept_ids, attention_masks, token_type_ids], labels\n        else:\n            return [input_ids, dept_ids, attention_masks, token_type_ids]\n\n    def on_epoch_end(self):\n        if self.shuffle:\n            np.random.RandomState(42).shuffle(self.indexes)\n",
"execution_count": 9,
"outputs": []
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\ndef macro_double_soft_f1(y, y_hat):\n    \n    y = tf.cast(y, tf.float32)\n    y_hat = tf.cast(y_hat, tf.float32)\n    tp = tf.reduce_sum(y_hat * y, axis=0)\n    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n    tn = tf.reduce_sum((1 - y_hat) * (1 - y), axis=0)\n    soft_f1_class1 = 2*tp / (2*tp + fn + fp + 1e-16)\n    soft_f1_class0 = 2*tn / (2*tn + fn + fp + 1e-16)\n    cost_class1 = 1 - soft_f1_class1 # reduce 1 - soft-f1_class1 in order to increase soft-f1 on class 1\n    cost_class0 = 1 - soft_f1_class0 # reduce 1 - soft-f1_class0 in order to increase soft-f1 on class 0\n    cost = 0.5 * (cost_class1 + cost_class0) # take into account both class 1 and class 0\n    macro_cost = tf.reduce_mean(cost) # average on all labels\n    return macro_cost\n\n",
"execution_count": 10,
"outputs": []
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "\ninput_ids = tf.keras.layers.Input(\n    shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n)\n\ninput_departments = tf.keras.Input(\n    shape = (4,), dtype=tf.float32, name='input_depts'\n)\n\n# Attention masks indicates to the model which tokens should be attended to.\nattention_masks = tf.keras.layers.Input(\n    shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n)\n\n# Token type ids are binary masks identifying different sequences in the model.\ntoken_type_ids = tf.keras.layers.Input(\n    shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n)\n\n# Loading pretrained BERT model.\n# bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\nbert_model = transformers.TFBertModel.from_pretrained(bert_model_name, from_pt=True, config = config)\n# Freeze the BERT model to reuse the pretrained features without modifying them.\nbert_model.trainable = False\n\nsequence_output, pooled_output = bert_model(\n    input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n)\n\n# Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\nbi_lstm = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(64, return_sequences=True)\n)(sequence_output)\n\n# Applying hybrid pooling approach to bi_lstm sequence output.\navg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\nmax_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\nconcat = tf.keras.layers.concatenate([avg_pool, max_pool, input_departments])\ndropout = tf.keras.layers.Dropout(0.3)(concat)\noutput = tf.keras.layers.Dense(25, activation=\"sigmoid\")(dropout)\nmodel = tf.keras.models.Model(\n    inputs=[input_ids, input_departments, attention_masks, token_type_ids], outputs=output\n)\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss='binary_crossentropy',\n    metrics=[\"acc\", f1_m],\n)\n\nmodel.summary()\n",
"execution_count": 11,
"outputs": [
{
"output_type": "stream",
"text": "Model: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_ids (InputLayer)          [(None, 512)]        0                                            \n__________________________________________________________________________________________________\nattention_masks (InputLayer)    [(None, 512)]        0                                            \n__________________________________________________________________________________________________\ntoken_type_ids (InputLayer)     [(None, 512)]        0                                            \n__________________________________________________________________________________________________\ntf_bert_model (TFBertModel)     ((None, 512, 768), ( 109918464   input_ids[0][0]                  \n                                                                 attention_masks[0][0]            \n                                                                 token_type_ids[0][0]             \n__________________________________________________________________________________________________\nbidirectional (Bidirectional)   (None, 512, 128)     426496      tf_bert_model[0][0]              \n__________________________________________________________________________________________________\nglobal_average_pooling1d (Globa (None, 128)          0           bidirectional[0][0]              \n__________________________________________________________________________________________________\nglobal_max_pooling1d (GlobalMax (None, 128)          0           bidirectional[0][0]              \n__________________________________________________________________________________________________\ninput_depts (InputLayer)        [(None, 4)]          0                                            \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 260)          0           global_average_pooling1d[0][0]   \n                                                                 global_max_pooling1d[0][0]       \n                                                                 input_depts[0][0]                \n__________________________________________________________________________________________________\ndropout_37 (Dropout)            (None, 260)          0           concatenate[0][0]                \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 25)           6525        dropout_37[0][0]                 \n==================================================================================================\nTotal params: 110,351,485\nTrainable params: 433,021\nNon-trainable params: 109,918,464\n__________________________________________________________________________________________________\n",
"name": "stdout"
}
]
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "# debug",
"execution_count": 12,
"outputs": []
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "val_data = BertSemanticDataGenerator(\n    test_X['ABSTRACT'].values.astype(\"str\"),\n    test_X.iloc[:,1:6].values.astype(\"int32\"),\n    test_y,\n    batch_size=batch_size,\n    shuffle=False,\n)\n\n\ndata = BertSemanticDataGenerator(\n    X['ABSTRACT'].values.astype(\"str\"),\n    X.iloc[:,1:6].values.astype(\"int32\"),\n    y,\n    batch_size=batch_size,\n    shuffle=True,\n)\n\n\n\n\nprint(\"training model\")\nhistory = model.fit(\n    data,\n    validation_data=val_data,\n    shuffle=True,\n    epochs=epochs,\n)\n\nK.clear_session()\ngc.collect()\nprint(\"sleeping\")\n# time.sleep(60)\n",
"execution_count": 13,
"outputs": [
{
"output_type": "stream",
"text": "training model\nEpoch 1/5\n437/437 [==============================] - 427s 977ms/step - loss: 0.1308 - acc: 0.4322 - f1_m: 0.3825 - val_loss: 0.0825 - val_acc: 0.6135 - val_f1_m: 0.6540\nEpoch 2/5\n437/437 [==============================] - 422s 965ms/step - loss: 0.0836 - acc: 0.6037 - f1_m: 0.6367 - val_loss: 0.0670 - val_acc: 0.6480 - val_f1_m: 0.7173\nEpoch 3/5\n437/437 [==============================] - 422s 967ms/step - loss: 0.0733 - acc: 0.6427 - f1_m: 0.6936 - val_loss: 0.0621 - val_acc: 0.6667 - val_f1_m: 0.7488\nEpoch 4/5\n437/437 [==============================] - 422s 966ms/step - loss: 0.0680 - acc: 0.6570 - f1_m: 0.7210 - val_loss: 0.0568 - val_acc: 0.6943 - val_f1_m: 0.7685\nEpoch 5/5\n437/437 [==============================] - 422s 967ms/step - loss: 0.0638 - acc: 0.6703 - f1_m: 0.7416 - val_loss: 0.0523 - val_acc: 0.7029 - val_f1_m: 0.7941\nsleeping\n",
"name": "stdout"
}
]
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "\nepochs = 2\nbatch_size = 128\nval_data = BertSemanticDataGenerator(\n    test_X['ABSTRACT'].values.astype(\"str\"),\n    test_X.iloc[:,1:6].values.astype(\"int32\"),\n    test_y,\n    batch_size=batch_size,\n    shuffle=False,\n)\n\n\ndata = BertSemanticDataGenerator(\n    X['ABSTRACT'].values.astype(\"str\"),\n    X.iloc[:,1:6].values.astype(\"int32\"),\n    y,\n    batch_size=batch_size,\n    shuffle=True,\n)\n\nprint(\"training trainable bert model\")\n\nhistory = model.fit(\n    data,\n    validation_data=val_data,\n    epochs=epochs,\n)\n\n\nval_data = BertSemanticDataGenerator(\n    test_X['ABSTRACT'].values.astype(\"str\"),\n    test_X.iloc[:,1:6].values.astype(\"int32\"),\n    test_y,\n    batch_size=1,\n    shuffle=False,\n)\n\n\nprint(\"predicting validation data\")\npreds = model.predict_generator(val_data, verbose=1, use_multiprocessing=True)\n\n\n\n\nprint(\"evaluating validation data\")\npkl.dump(test_y, open(\"val_original.pkl\", \"wb\"))\npkl.dump(preds, open(\"val_preds.pkl\", \"wb\"))\nmodel.evaluate(val_data, verbose=1)\n\n\ntdf = pd.read_csv(\"/kaggle/input/hacklive-3-guided-hackathon-nlp/Test.csv\")\n\n\nval_data = BertSemanticDataGenerator(\n    tdf['ABSTRACT'].values.astype(\"str\"),\n    tdf.iloc[:,2:6].values.astype(\"int32\"),\n    batch_size=2,\n    labels=None,\n    shuffle=False,\n    include_targets=False\n)\n\n\nprint(\"predicting test data\")\npreds = model.predict_generator(val_data, verbose=1, use_multiprocessing=True)\n\n\nprint(\"final predictions\")\nprint(preds)\n\npkl.dump(preds, open('pred_proba.pkl', 'wb'))\n\n",
"execution_count": 14,
"outputs": [
{
"output_type": "stream",
"text": "training trainable bert model\nEpoch 1/2\n109/109 [==============================] - 394s 4s/step - loss: 0.0581 - acc: 0.6907 - f1_m: 0.7683 - val_loss: 0.0484 - val_acc: 0.7113 - val_f1_m: 0.8085\nEpoch 2/2\n109/109 [==============================] - 393s 4s/step - loss: 0.0558 - acc: 0.6990 - f1_m: 0.7795 - val_loss: 0.0470 - val_acc: 0.7210 - val_f1_m: 0.8105\npredicting validation data\n2801/2801 [==============================] - 188s 67ms/step\nevaluating validation data\n2801/2801 [==============================] - 182s 65ms/step - loss: 0.0466 - acc: 0.7222 - f1_m: 0.8090\npredicting test data\n3001/3001 [==============================] - 274s 91ms/step\nfinal predictions\n[[4.7558631e-05 1.1307270e-02 9.6181147e-02 ... 4.7212100e-04\n  1.6809901e-04 2.0960410e-04]\n [1.4751501e-04 7.1922654e-01 2.5488198e-02 ... 4.9949839e-04\n  4.0903818e-04 1.7602278e-02]\n [3.0159997e-02 2.8757462e-03 3.7754685e-04 ... 1.3196630e-03\n  2.0742104e-03 2.1739595e-03]\n ...\n [2.5280016e-05 2.0353671e-03 1.2329185e-01 ... 7.8605844e-05\n  3.2520323e-04 4.5991065e-03]\n [7.3144166e-04 1.2549895e-01 4.4030617e-03 ... 2.4952746e-03\n  3.0379283e-04 3.4644967e-03]\n [1.4542748e-04 1.1508097e-03 7.6570007e-04 ... 3.7750635e-01\n  8.7890394e-02 2.0964089e-04]]\n",
"name": "stdout"
}
]
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "\n\npred = 'val_preds.pkl'\ny = 'val_original.pkl'\nfinal_pred = 'pred_proba.pkl'\n\n",
"execution_count": 15,
"outputs": []
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "\npred = pkl.load(open(pred, 'rb'))\n\ny = pkl.load(open(y, 'rb'))\ny = y[0:0 + pred.shape[0]]\n\nthresholds = [0] * 25\ncounts = [0] * 25\ny_trues = [0] * 25\n\ndef get_current_loss(p_col, true_col, th):\n    p_col = sum(p_col > th) / len(p_col)\n    true_col = sum(true_col) / len(true_col)\n    return abs(p_col - true_col)\n\n\nfor col in range(25):\n    current_loss = sys.maxsize\n    temp_loss = sys.maxsize\n    for threshold in np.linspace(0, 1, 40, endpoint=False):\n        pred_col = np.take(pred, col, axis=1)\n        y_col = np.take(y, col, axis=1)\n        temp_loss = get_current_loss(pred_col, y_col, threshold)\n        if  temp_loss < current_loss:\n            thresholds[col] = threshold\n            current_loss = temp_loss\n            counts[col] = sum(pred_col > threshold) / len(pred_col)\n            y_trues[col] = sum(y_col) / len(y_col)\n            # print(col, current_loss, thresholds[col], counts[col], y_trues[col])\n\n\nprint(thresholds)\nprint(counts)\nprint(y_trues)\n",
"execution_count": 16,
"outputs": [
{
"output_type": "stream",
"text": "[0.3375, 0.28750000000000003, 0.30000000000000004, 0.6125, 0.48750000000000004, 0.3125, 0.47500000000000003, 0.2625, 0.23750000000000002, 0.36250000000000004, 0.28750000000000003, 0.275, 0.3125, 0.4375, 0.4375, 0.4, 0.28750000000000003, 0.325, 0.4, 0.5125000000000001, 0.41250000000000003, 0.28750000000000003, 0.4, 0.4625, 0.4]\n[0.04248482684755445, 0.04141378079257408, 0.09532309889325241, 0.03641556586933238, 0.04641199571581578, 0.07283113173866476, 0.047483041770796146, 0.03248839700107105, 0.03855765797929311, 0.035344519814352014, 0.027490182077829346, 0.026776151374509102, 0.034987504462691894, 0.26347732952516956, 0.0567654409139593, 0.04212781149589433, 0.026062120671188863, 0.04891110317743663, 0.02034987504462692, 0.07640128525526597, 0.043198857550874686, 0.0367725812209925, 0.0649767940021421, 0.03784362727597287, 0.0467690110674759]\n[0.04248482684755445, 0.04212781149589433, 0.09460906818993217, 0.03641556586933238, 0.04641199571581578, 0.07247411638700464, 0.048197072474116386, 0.03213138164941092, 0.038914673330953234, 0.035344519814352014, 0.027490182077829346, 0.026776151374509102, 0.034987504462691894, 0.26347732952516956, 0.057122456265619424, 0.0417707961442342, 0.026062120671188863, 0.047840057122456266, 0.02034987504462692, 0.07640128525526597, 0.043198857550874686, 0.03605855051767226, 0.06569082470546234, 0.03784362727597287, 0.0467690110674759]\n",
"name": "stdout"
}
]
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "\ndf = pd.read_csv('/kaggle/input/hacklive-3-guided-hackathon-nlp/Train.csv')\n\ny = df.iloc[:, 6: 6 + 25]\nLABELS = y.columns\n\n\ntdf = pd.read_csv(\"/kaggle/input/hacklive-3-guided-hackathon-nlp/Test.csv\")\ni = pkl.load(open(final_pred, \"rb\"))\n\nfor col in range(len(LABELS)):\n    tdf[LABELS[col]] = [1 if x[col] > thresholds[col] else 0 for x in i]\n    \n\ntdf.drop(columns=['ABSTRACT', 'Computer Science', 'Mathematics', 'Physics', 'Statistics']).to_csv(\"final1.csv\", index=False)",
"execution_count": 17,
"outputs": []
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "from IPython.display import FileLink\nfilename = \"final.csv\"\nFileLink(filename)",
"execution_count": 18,
"outputs": [
{
"output_type": "execute_result",
"execution_count": 18,
"data": {
"text/plain": "/kaggle/working/final.csv",
"text/html": "<a href='final.csv' target='_blank'>final.csv</a><br>"
},
"metadata": {}
}
]
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "\npred = 'val_preds.pkl'\ny = 'val_original.pkl'\nfinal_pred = 'pred_proba.pkl'",
"execution_count": 19,
"outputs": []
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "FileLink(pred)",
"execution_count": 20,
"outputs": [
{
"output_type": "execute_result",
"execution_count": 20,
"data": {
"text/plain": "/kaggle/working/val_preds.pkl",
"text/html": "<a href='val_preds.pkl' target='_blank'>val_preds.pkl</a><br>"
},
"metadata": {}
}
]
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "FileLink(y)",
"execution_count": 21,
"outputs": [
{
"output_type": "execute_result",
"execution_count": 21,
"data": {
"text/plain": "/kaggle/working/val_original.pkl",
"text/html": "<a href='val_original.pkl' target='_blank'>val_original.pkl</a><br>"
},
"metadata": {}
}
]
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "FileLink(final_pred)",
"execution_count": 22,
"outputs": [
{
"output_type": "execute_result",
"execution_count": 22,
"data": {
"text/plain": "/kaggle/working/pred_proba.pkl",
"text/html": "<a href='pred_proba.pkl' target='_blank'>pred_proba.pkl</a><br>"
},
"metadata": {}
}
]
},
{
"metadata": {
"trusted": true
},
"cell_type": "code",
"source": "",
"execution_count": null,
"outputs": []
}
],
"metadata": {
"kernelspec": {
"name": "python3",
"display_name": "Python 3",
"language": "python"
},
"language_info": {
"name": "python",
"version": "3.7.6",
"mimetype": "text/x-python",
"codemirror_mode": {
"name": "ipython",
"version": 3
},
"pygments_lexer": "ipython3",
"nbconvert_exporter": "python",
"file_extension": ".py"
}
},
"nbformat": 4,
"nbformat_minor": 4
}
