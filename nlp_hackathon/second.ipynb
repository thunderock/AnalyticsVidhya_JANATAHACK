{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as K\n",
    "import pickle as pkl\n",
    "import gc\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 400\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "df = pd.read_csv('Train.csv')\n",
    "\n",
    "X = df.iloc[:, 1: 6]\n",
    "y = df.iloc[:, 6: 6 + 25]\n",
    "\n",
    "LABELS = y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, y, shuffle=True, test_size=.2)\n",
    "\n",
    "train_X.shape, train_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_X, train_y\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)\n",
    "# train_y = train_y.reset_index(drop=True)\n",
    "test_y = test_y.reset_index(drop=True)\n",
    "# train_X = train_X.reset_index(drop=True)\n",
    "test_X = test_X.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_y = train_y.values\n",
    "test_y = test_y.values\n",
    "y = y.values\n",
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Generates batches of data.\n",
    "\n",
    "    Args:\n",
    "        sentence_pairs: Array of premise and hypothesis input sentences.\n",
    "        labels: Array of labels.\n",
    "        batch_size: Integer batch size.\n",
    "        shuffle: boolean, whether to shuffle the data.\n",
    "        include_targets: boolean, whether to incude the labels.\n",
    "\n",
    "    Returns:\n",
    "        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n",
    "        (or just `[input_ids, attention_mask, `token_type_ids]`\n",
    "         if `include_targets=False`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sentences,\n",
    "        depts,\n",
    "        labels,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        include_targets=True,\n",
    "    ):\n",
    "        self.sentences = sentences\n",
    "        self.depts = depts\n",
    "        self.labels = labels\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.include_targets = include_targets\n",
    "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "            \"bert-base-uncased\", do_lower_case=True\n",
    "        )\n",
    "        self.indexes = np.arange(len(self.sentences))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        sentences = self.sentences[indexes]\n",
    "\n",
    "        encoded = self.tokenizer.batch_encode_plus(\n",
    "            sentences.tolist(),\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            pad_to_max_length=True,\n",
    "            return_tensors=\"tf\",\n",
    "        )\n",
    "\n",
    "        \n",
    "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "        \n",
    "        dept_ids = np.array(self.depts[indexes], dtype=\"int32\")\n",
    "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "        \n",
    "        if self.include_targets:\n",
    "            labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
    "            return [input_ids, dept_ids, attention_masks, token_type_ids], labels\n",
    "        else:\n",
    "            return [input_ids, dept_ids, attention_masks, token_type_ids]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.RandomState(42).shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.iloc[:,1:6].columns, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_keras():\n",
    "    sess = KK.get_session()\n",
    "    KK.clear_session()\n",
    "    sess.close()\n",
    "    sess = KK.get_session()\n",
    "\n",
    "#     try:\n",
    "#         del classifier # this is from global space - change this as you need\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "    print(gc.collect()) # if it does something you should see a number as output\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tf.Session(config=config))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    \n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n",
    "    )\n",
    "\n",
    "    input_departments = tf.keras.Input(\n",
    "        shape = (4,), dtype=tf.float32, name='input_depts'\n",
    "    )\n",
    "\n",
    "    # Attention masks indicates to the model which tokens should be attended to.\n",
    "    attention_masks = tf.keras.layers.Input(\n",
    "        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n",
    "    )\n",
    "\n",
    "    # Token type ids are binary masks identifying different sequences in the model.\n",
    "    token_type_ids = tf.keras.layers.Input(\n",
    "        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
    "    )\n",
    "\n",
    "    # Loading pretrained BERT model.\n",
    "    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # Freeze the BERT model to reuse the pretrained features without modifying them.\n",
    "    bert_model.trainable = False\n",
    "\n",
    "    sequence_output, pooled_output = bert_model(\n",
    "        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
    "    )\n",
    "\n",
    "    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n",
    "    bi_lstm = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True)\n",
    "    )(sequence_output)\n",
    "\n",
    "    # Applying hybrid pooling approach to bi_lstm sequence output.\n",
    "    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n",
    "    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n",
    "    concat = tf.keras.layers.concatenate([avg_pool, max_pool, input_departments])\n",
    "    dropout = tf.keras.layers.Dropout(0.3)(concat)\n",
    "    output = tf.keras.layers.Dense(25, activation=\"softmax\")(dropout)\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=[input_ids, input_departments, attention_masks, token_type_ids], outputs=output\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"acc\", f1_m],\n",
    "    )\n",
    "    \n",
    "print(f\"Strategy: {strategy}\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_data = BertSemanticDataGenerator(\n",
    "    test_X['ABSTRACT'].values.astype(\"str\"),\n",
    "    test_X.iloc[:,1:6].values.astype(\"int32\"),\n",
    "    test_y,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "data = BertSemanticDataGenerator(\n",
    "    X['ABSTRACT'].values.astype(\"str\"),\n",
    "    X.iloc[:,1:6].values.astype(\"int32\"),\n",
    "    y,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.iloc[:,1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    data,\n",
    "    validation_data=val_data,\n",
    "    shuffle=True,\n",
    "    epochs=epochs,\n",
    "#     use_multiprocessing=True,\n",
    "#     workers=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "# Unfreeze the bert_model.\n",
    "bert_model.trainable = True\n",
    "# Recompile the model to make the change effective.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"acc\", f1_m],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "val_data = BertSemanticDataGenerator(\n",
    "    test_X['ABSTRACT'].values.astype(\"str\"),\n",
    "    test_X.iloc[:,1:6].values.astype(\"int32\"),\n",
    "    test_y,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "data = BertSemanticDataGenerator(\n",
    "    X['ABSTRACT'].values.astype(\"str\"),\n",
    "    X.iloc[:,1:6].values.astype(\"int32\"),\n",
    "    y,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    data,\n",
    "    validation_data=val_data,\n",
    "    epochs=epochs,\n",
    "    #use_multiprocessing=True,\n",
    "    #workers=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = BertSemanticDataGenerator(\n",
    "    test_X['ABSTRACT'].values.astype(\"str\"),\n",
    "    test_X.iloc[:,1:6].values.astype(\"int32\"),\n",
    "    test_y,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "preds = model.predict_generator(val_data, verbose=1, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(test_y, open(\"val_original.pkl\", \"rb\"))\n",
    "pkl.dump(preds, open(\"val_preds.pkl\", \"rb\"))\n",
    "model.evaluate(val_data, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_similarity(sentence, depts):\n",
    "# #     sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n",
    "#     test_data = BertSemanticDataGenerator(\n",
    "#         np.array(sentence), depts, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
    "#     )\n",
    "\n",
    "#     proba = model.predict(test_data)[0]\n",
    "#     idx = np.argmax(proba)\n",
    "#     proba = f\"{proba[idx]: .2f}%\"\n",
    "#     pred = labels[idx]\n",
    "#     return pred, proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = train_X.iloc[0]['ABSTRACT']\n",
    "# d = train_X.iloc[:,1:6].iloc[0].values\n",
    "# check_similarity(s, d)\n",
    "# # sentence2 = \"Two women are standing with their eyes closed.\"\n",
    "# # check_similarity(sentence1, sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence1 = \"A smiling costumed woman is holding an umbrella\"\n",
    "# sentence2 = \"A happy woman in a fairy costume holds an umbrella\"\n",
    "# check_similarity(sentence1, sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence1 = \"A soccer game with multiple males playing\"\n",
    "# sentence2 = \"Some men are playing a sport\"\n",
    "# check_similarity(sentence1, sentence2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"model.h5\")\n",
    "# print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.read_csv(\"Test.csv\")\n",
    "# df = df.iloc[:, 1: 6]\n",
    "tdf.iloc[:, 2: 6].head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_data = BertSemanticDataGenerator(\n",
    "    tdf['ABSTRACT'].values.astype(\"str\"),\n",
    "    tdf.iloc[:,2:6].values.astype(\"int32\"),\n",
    "    # None,\n",
    "    batch_size=2,\n",
    "    labels=None,\n",
    "    shuffle=False, \n",
    "    include_targets=False\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_generator(val_data, verbose=1, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape, tdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = {}\n",
    "for x in y:\n",
    "    ones = 0\n",
    "    for xx in x:\n",
    "        if xx == 1: ones += 1\n",
    "    if ones in cnt: cnt[ones] += 1\n",
    "    else: cnt[ones] = 1\n",
    "for x in cnt: \n",
    "    print(x, cnt[x] / sum(cnt.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "THRESHOLD = .17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt2 = {}\n",
    "for x in preds:\n",
    "    ones = 0\n",
    "    for xx in x:\n",
    "        if xx > THRESHOLD: ones += 1\n",
    "    if ones in cnt2: cnt2[ones] += 1\n",
    "    else: cnt2[ones] = 1\n",
    "for x in cnt2: \n",
    "    print(x, cnt2[x] / sum(cnt2.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in range(len(LABELS)):\n",
    "    tdf[LABELS[col]] = [1 if x[col] > THRESHOLD else 0 for x in preds]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.drop(columns=['ABSTRACT', 'Computer Science', 'Mathematics', 'Physics', 'Statistics']).to_csv(\"final.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pkl.dump(preds, open('pred_proba.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
