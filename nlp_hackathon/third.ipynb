{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/hacklive-3-guided-hackathon-nlp/Test.csv\n",
      "/kaggle/input/hacklive-3-guided-hackathon-nlp/Tags.csv\n",
      "/kaggle/input/hacklive-3-guided-hackathon-nlp/Train.csv\n",
      "/kaggle/input/hacklive-3-guided-hackathon-nlp/SampleSubmission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as K\n",
    "import pickle as pkl\n",
    "import gc\n",
    "import logging\n",
    "import warnings\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length = 512\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "df = pd.read_csv('Train.csv')\n",
    "\n",
    "X = df.iloc[:, 1: 6]\n",
    "y = df.iloc[:, 6: 6 + 25]\n",
    "\n",
    "LABELS = y.columns\n",
    "\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, shuffle=True, test_size=.2)\n",
    "train_X.shape, train_y.shape, test_X.shape, test_y.shape\n",
    "\n",
    "\n",
    "del train_X, train_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)\n",
    "test_y = test_y.reset_index(drop=True)\n",
    "test_X = test_X.reset_index(drop=True)\n",
    "\n",
    "\n",
    "test_y = test_y.values\n",
    "y = y.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sentences,\n",
    "        depts,\n",
    "        labels,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        include_targets=True,\n",
    "    ):\n",
    "        self.sentences = sentences\n",
    "        self.depts = depts\n",
    "        self.labels = labels\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.include_targets = include_targets\n",
    "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "            \"bert-base-uncased\", do_lower_case=True, max_length=2048\n",
    "            )\n",
    "        self.indexes = np.arange(len(self.sentences))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        sentences = self.sentences[indexes]\n",
    "\n",
    "        encoded = self.tokenizer.batch_encode_plus(\n",
    "            sentences.tolist(),\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            pad_to_max_length=True,\n",
    "            return_tensors=\"tf\",\n",
    "        )\n",
    "\n",
    "\n",
    "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "\n",
    "        dept_ids = np.array(self.depts[indexes], dtype=\"int32\")\n",
    "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "\n",
    "        if self.include_targets:\n",
    "            labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
    "            return [input_ids, dept_ids, attention_masks, token_type_ids], labels\n",
    "        else:\n",
    "            return [input_ids, dept_ids, attention_masks, token_type_ids]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.RandomState(42).shuffle(self.indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2963accee394da38778ec50ad54456d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e41a2a8466420d801ff32a6bba27e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=536063208.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_masks (InputLayer)    [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 512, 768), ( 109482240   input_ids[0][0]                  \n",
      "                                                                 attention_masks[0][0]            \n",
      "                                                                 token_type_ids[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 512, 128)     426496      tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 128)          0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_depts (InputLayer)        [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 260)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_max_pooling1d[0][0]       \n",
      "                                                                 input_depts[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 260)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 25)           6525        dropout_37[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 109,915,261\n",
      "Trainable params: 433,021\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_ids = tf.keras.layers.Input(\n",
    "    shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n",
    ")\n",
    "\n",
    "input_departments = tf.keras.Input(\n",
    "    shape = (4,), dtype=tf.float32, name='input_depts'\n",
    ")\n",
    "\n",
    "# Attention masks indicates to the model which tokens should be attended to.\n",
    "attention_masks = tf.keras.layers.Input(\n",
    "    shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n",
    ")\n",
    "\n",
    "# Token type ids are binary masks identifying different sequences in the model.\n",
    "token_type_ids = tf.keras.layers.Input(\n",
    "    shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
    ")\n",
    "\n",
    "# Loading pretrained BERT model.\n",
    "bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Freeze the BERT model to reuse the pretrained features without modifying them.\n",
    "bert_model.trainable = False\n",
    "\n",
    "sequence_output, pooled_output = bert_model(\n",
    "    input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
    ")\n",
    "\n",
    "# Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n",
    "bi_lstm = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True)\n",
    ")(sequence_output)\n",
    "\n",
    "# Applying hybrid pooling approach to bi_lstm sequence output.\n",
    "avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n",
    "max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n",
    "concat = tf.keras.layers.concatenate([avg_pool, max_pool, input_departments])\n",
    "dropout = tf.keras.layers.Dropout(0.3)(concat)\n",
    "output = tf.keras.layers.Dense(25, activation=\"softmax\")(dropout)\n",
    "model = tf.keras.models.Model(\n",
    "    inputs=[input_ids, input_departments, attention_masks, token_type_ids], outputs=output\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"acc\", f1_m],\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e1488cbbce4b5da9988f5d786a0f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training model\n",
      "Epoch 1/4\n",
      "162/437 [==========>...................] - ETA: 3:45 - loss: 0.1465 - acc: 0.3262 - f1_m: 0.1214"
     ]
    }
   ],
   "source": [
    "val_data = BertSemanticDataGenerator(\n",
    "    test_X['ABSTRACT'].values.astype(\"str\"),\n",
    "    test_X.iloc[:,1:6].values.astype(\"int32\"),\n",
    "    test_y,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "data = BertSemanticDataGenerator(\n",
    "    X['ABSTRACT'].values.astype(\"str\"),\n",
    "    X.iloc[:,1:6].values.astype(\"int32\"),\n",
    "    y,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"training model\")\n",
    "history = model.fit(\n",
    "    data,\n",
    "    validation_data=val_data,\n",
    "    shuffle=True,\n",
    "    epochs=epochs,\n",
    ")\n",
    "\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "print(\"sleeping\")\n",
    "# time.sleep(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 2\n",
    "batch_size = 2\n",
    "val_data = BertSemanticDataGenerator(\n",
    "    test_X['ABSTRACT'].values.astype(\"str\"),\n",
    "    test_X.iloc[:,1:6].values.astype(\"int32\"),\n",
    "    test_y,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "data = BertSemanticDataGenerator(\n",
    "    X['ABSTRACT'].values.astype(\"str\"),\n",
    "    X.iloc[:,1:6].values.astype(\"int32\"),\n",
    "    y,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "print(\"training trainable bert model\")\n",
    "\n",
    "# history = model.fit(\n",
    "#     data,\n",
    "#     validation_data=val_data,\n",
    "#     epochs=epochs,\n",
    "# )\n",
    "\n",
    "\n",
    "val_data = BertSemanticDataGenerator(\n",
    "    test_X['ABSTRACT'].values.astype(\"str\"),\n",
    "    test_X.iloc[:,1:6].values.astype(\"int32\"),\n",
    "    test_y,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"predicting validation data\")\n",
    "preds = model.predict_generator(val_data, verbose=1, use_multiprocessing=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"evaluating validation data\")\n",
    "pkl.dump(test_y, open(\"val_original.pkl\", \"wb\"))\n",
    "pkl.dump(preds, open(\"val_preds.pkl\", \"wb\"))\n",
    "model.evaluate(val_data, verbose=1)\n",
    "\n",
    "\n",
    "tdf = pd.read_csv(\"Test.csv\")\n",
    "\n",
    "\n",
    "val_data = BertSemanticDataGenerator(\n",
    "    tdf['ABSTRACT'].values.astype(\"str\"),\n",
    "    tdf.iloc[:,2:6].values.astype(\"int32\"),\n",
    "    batch_size=2,\n",
    "    labels=None,\n",
    "    shuffle=False,\n",
    "    include_targets=False\n",
    ")\n",
    "\n",
    "\n",
    "print(\"predicting test data\")\n",
    "preds = model.predict_generator(val_data, verbose=1, use_multiprocessing=True)\n",
    "\n",
    "\n",
    "print(\"final predictions\")\n",
    "print(preds)\n",
    "\n",
    "pkl.dump(preds, open('pred_proba.pkl', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pred = 'val_preds.pkl'\n",
    "y = 'val_original.pkl'\n",
    "final_pred = 'pred_proba.pkl'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred = pkl.load(open(pred, 'rb'))\n",
    "\n",
    "y = pkl.load(open(y, 'rb'))\n",
    "y = y[0:0 + pred.shape[0]]\n",
    "\n",
    "thresholds = [0] * 25\n",
    "counts = [0] * 25\n",
    "y_trues = [0] * 25\n",
    "\n",
    "def get_current_loss(p_col, true_col, th):\n",
    "    p_col = sum(p_col > th) / len(p_col)\n",
    "    true_col = sum(true_col) / len(true_col)\n",
    "    return abs(p_col - true_col)\n",
    "\n",
    "\n",
    "for col in range(25):\n",
    "    current_loss = sys.maxsize\n",
    "    temp_loss = sys.maxsize\n",
    "    for threshold in np.linspace(0, 1, 20, endpoint=False):\n",
    "        pred_col = np.take(pred, col, axis=1)\n",
    "        y_col = np.take(y, col, axis=1)\n",
    "        temp_loss = get_current_loss(pred_col, y_col, threshold)\n",
    "        if  temp_loss < current_loss:\n",
    "            thresholds[col] = threshold\n",
    "            current_loss = temp_loss\n",
    "            counts[col] = sum(pred_col > threshold) / len(pred_col)\n",
    "            y_trues[col] = sum(y_col) / len(y_col)\n",
    "            # print(col, current_loss, thresholds[col], counts[col], y_trues[col])\n",
    "\n",
    "\n",
    "print(thresholds)\n",
    "print(counts)\n",
    "print(y_trues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('Train.csv')\n",
    "\n",
    "y = df.iloc[:, 6: 6 + 25]\n",
    "LABELS = y.columns\n",
    "\n",
    "\n",
    "tdf = pd.read_csv(\"Test.csv\")\n",
    "i = pkl.load(open(final_pred, \"rb\"))\n",
    "\n",
    "for col in range(len(LABELS)):\n",
    "    tdf[LABELS[col]] = [1 if x[col] > thresholds[col] else 0 for x in i]\n",
    "    \n",
    "\n",
    "tdf.drop(columns=['ABSTRACT', 'Computer Science', 'Mathematics', 'Physics', 'Statistics']).to_csv(\"final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "filename = \"final.csv\"\n",
    "FileLink(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
