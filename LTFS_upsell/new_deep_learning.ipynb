{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow import keras\n",
    "import os, logging\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import pickle as pkl\n",
    "from tensorflow.keras.layers import Concatenate, LSTM, Dense, TimeDistributed, Embedding, Bidirectional, Dropout, concatenate, SpatialDropout1D, GlobalMaxPooling1D, Reshape, MaxPooling1D, Flatten, Conv1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_training.csv\")\n",
    "tdf = pd.read_csv(\"final_testing.csv\")\n",
    "train_df = pd.read_csv(\"Train/cleaned_train.csv\")\n",
    "test_df = pd.read_csv(\"Test/cleaned_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(df.isnull().sum() == 0), all(tdf.isnull().sum() == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['SELF-INDICATOR', 'MATCH-TYPE', 'ACCT-TYPE', 'CONTRIBUTOR-TYPE',\n",
    "       'OWNERSHIP-IND', 'ACCOUNT-STATUS', 'INSTALLMENT-TYPE',\n",
    "       'ASSET_CLASS', 'INSTALLMENT-FREQUENCY',\n",
    "       'DPD - HIST', 'DATE-REPORTED_0', 'DATE-REPORTED_1', 'DATE-REPORTED_2',\n",
    " 'DATE-REPORTED_3','DATE-REPORTED_4','DISBURSED-DT_0',\n",
    " 'DISBURSED-DT_1',\n",
    " 'DISBURSED-DT_2',\n",
    " 'DISBURSED-DT_3',\n",
    " 'DISBURSED-DT_4',\n",
    " 'CLOSE-DT_0',\n",
    " 'CLOSE-DT_1',\n",
    " 'CLOSE-DT_2',\n",
    " 'CLOSE-DT_3',\n",
    " 'CLOSE-DT_4',\n",
    " 'LAST-PAYMENT-DATE_0',\n",
    " 'LAST-PAYMENT-DATE_1',\n",
    " 'LAST-PAYMENT-DATE_2',\n",
    " 'LAST-PAYMENT-DATE_3',\n",
    " 'LAST-PAYMENT-DATE_4']\n",
    "reg_cols = ['REPORTED DATE - HIST_0', 'REPORTED DATE - HIST_1', 'REPORTED DATE - HIST_2', 'REPORTED DATE - HIST_3', \n",
    "            'REPORTED DATE - HIST_4', 'REPORTED DATE - HIST_5', 'REPORTED DATE - HIST_6', 'REPORTED DATE - HIST_7', \n",
    "            'REPORTED DATE - HIST_8', 'REPORTED DATE - HIST_9', 'REPORTED DATE - HIST_10', 'REPORTED DATE - HIST_11', \n",
    "            'REPORTED DATE - HIST_12', 'REPORTED DATE - HIST_13', 'REPORTED DATE - HIST_14', 'REPORTED DATE - HIST_15', \n",
    "            'REPORTED DATE - HIST_16', 'REPORTED DATE - HIST_17', 'REPORTED DATE - HIST_18', 'REPORTED DATE - HIST_19', \n",
    "            'REPORTED DATE - HIST_20', 'REPORTED DATE - HIST_21', 'REPORTED DATE - HIST_22', 'REPORTED DATE - HIST_23', \n",
    "            'REPORTED DATE - HIST_24', 'REPORTED DATE - HIST_25', 'REPORTED DATE - HIST_26', 'REPORTED DATE - HIST_27', \n",
    "            'REPORTED DATE - HIST_28', 'REPORTED DATE - HIST_29', 'REPORTED DATE - HIST_30', 'REPORTED DATE - HIST_31', \n",
    "            'REPORTED DATE - HIST_32', 'REPORTED DATE - HIST_33', 'REPORTED DATE - HIST_34', 'REPORTED DATE - HIST_35', \n",
    "            'REPORTED DATE - HIST_36', 'REPORTED DATE - HIST_37', 'REPORTED DATE - HIST_38', 'REPORTED DATE - HIST_39', \n",
    "            'REPORTED DATE - HIST_40', 'REPORTED DATE - HIST_41', 'CUR BAL - HIST_0', 'CUR BAL - HIST_1', 'CUR BAL - HIST_2', \n",
    "            'CUR BAL - HIST_3', 'CUR BAL - HIST_4', 'CUR BAL - HIST_5', 'CUR BAL - HIST_6', 'CUR BAL - HIST_7', 'CUR BAL - HIST_8', \n",
    "            'CUR BAL - HIST_9', 'CUR BAL - HIST_10', 'CUR BAL - HIST_11', 'CUR BAL - HIST_12', 'CUR BAL - HIST_13', 'CUR BAL - HIST_14', \n",
    "            'CUR BAL - HIST_15', 'CUR BAL - HIST_16', 'CUR BAL - HIST_17', 'CUR BAL - HIST_18', 'CUR BAL - HIST_19', 'CUR BAL - HIST_20', \n",
    "            'CUR BAL - HIST_21', 'CUR BAL - HIST_22', 'CUR BAL - HIST_23', 'CUR BAL - HIST_24', 'CUR BAL - HIST_25', 'CUR BAL - HIST_26', \n",
    "            'CUR BAL - HIST_27', 'CUR BAL - HIST_28', 'CUR BAL - HIST_29', 'CUR BAL - HIST_30', 'CUR BAL - HIST_31', 'CUR BAL - HIST_32', \n",
    "            'CUR BAL - HIST_33', 'CUR BAL - HIST_34', 'CUR BAL - HIST_35', 'CUR BAL - HIST_36', 'CUR BAL - HIST_37', 'CUR BAL - HIST_38', \n",
    "            'CUR BAL - HIST_39', 'CUR BAL - HIST_40', 'CUR BAL - HIST_41', 'AMT OVERDUE - HIST_0', 'AMT OVERDUE - HIST_1', 'AMT OVERDUE - HIST_2', \n",
    "            'AMT OVERDUE - HIST_3', 'AMT OVERDUE - HIST_4', 'AMT OVERDUE - HIST_5', 'AMT OVERDUE - HIST_6', 'AMT OVERDUE - HIST_7', 'AMT OVERDUE - HIST_8', \n",
    "            'AMT OVERDUE - HIST_9', 'AMT OVERDUE - HIST_10', 'AMT OVERDUE - HIST_11', 'AMT OVERDUE - HIST_12', 'AMT OVERDUE - HIST_13', 'AMT OVERDUE - HIST_14', \n",
    "            'AMT OVERDUE - HIST_15', 'AMT OVERDUE - HIST_16', 'AMT OVERDUE - HIST_17', 'AMT OVERDUE - HIST_18', 'AMT OVERDUE - HIST_19', 'AMT OVERDUE - HIST_20', \n",
    "            'AMT OVERDUE - HIST_21', 'AMT OVERDUE - HIST_22', 'AMT OVERDUE - HIST_23', 'AMT OVERDUE - HIST_24', 'AMT OVERDUE - HIST_25', 'AMT OVERDUE - HIST_26', \n",
    "            'AMT OVERDUE - HIST_27', 'AMT OVERDUE - HIST_28', 'AMT OVERDUE - HIST_29', 'AMT OVERDUE - HIST_30', 'AMT OVERDUE - HIST_31', 'AMT OVERDUE - HIST_32', \n",
    "            'AMT OVERDUE - HIST_33', 'AMT OVERDUE - HIST_34', 'AMT OVERDUE - HIST_35', 'AMT OVERDUE - HIST_36', 'AMT OVERDUE - HIST_37', 'AMT OVERDUE - HIST_38', \n",
    "            'AMT OVERDUE - HIST_39', 'AMT OVERDUE - HIST_40', 'AMT OVERDUE - HIST_41', 'AMT PAID - HIST_0', 'AMT PAID - HIST_1', 'AMT PAID - HIST_2', 'AMT PAID - HIST_3', \n",
    "            'AMT PAID - HIST_4', 'AMT PAID - HIST_5', 'AMT PAID - HIST_6', 'AMT PAID - HIST_7', 'AMT PAID - HIST_8', 'AMT PAID - HIST_9', 'AMT PAID - HIST_10', 'AMT PAID - HIST_11', \n",
    "            'AMT PAID - HIST_12', 'AMT PAID - HIST_13', 'AMT PAID - HIST_14', 'AMT PAID - HIST_15', 'AMT PAID - HIST_16', 'AMT PAID - HIST_17', 'AMT PAID - HIST_18', \n",
    "            'AMT PAID - HIST_19', 'AMT PAID - HIST_20', 'AMT PAID - HIST_21', 'AMT PAID - HIST_22', 'AMT PAID - HIST_23', 'AMT PAID - HIST_24', 'AMT PAID - HIST_25', \n",
    "            'AMT PAID - HIST_26', 'AMT PAID - HIST_27', 'AMT PAID - HIST_28', 'AMT PAID - HIST_29', 'AMT PAID - HIST_30', 'AMT PAID - HIST_31', 'AMT PAID - HIST_32', \n",
    "            'AMT PAID - HIST_33', 'AMT PAID - HIST_34', 'AMT PAID - HIST_35', 'AMT PAID - HIST_36', 'AMT PAID - HIST_37', 'AMT PAID - HIST_38', 'AMT PAID - HIST_39', \n",
    "            'AMT PAID - HIST_40', 'AMT PAID - HIST_41','CREDIT-LIMIT/SANC AMT', 'DISBURSED-AMT/HIGH CREDIT', 'INSTALLMENT-AMT', 'CURRENT-BAL',\n",
    "        'OVERDUE-AMT', 'WRITE-OFF-AMT', 'TENURE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(cat_cols), len(reg_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[reg_cols].dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[cat_cols].dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tdf[reg_cols].dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tdf[cat_cols].dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_window = 420\n",
    "ts_feature_vector = 205\n",
    "max_array_size = 42\n",
    "label_encoder_dict = {}\n",
    "num_cores = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       [0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
       "        4.8000e+01],\n",
       "       [0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
       "        1.2000e+01],\n",
       "       ...,\n",
       "       [0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       [0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 2.3577e+04, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       [0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, df.columns != 'ID'].sample(10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_df_for_user(dframe):\n",
    "    \n",
    "    ret = dframe.loc[:, df.columns != 'ID'].values\n",
    "    ret = np.pad(ret,(((max_window - ret.shape[0]),0),(0, 0)), 'constant')\n",
    "#     ret = np.array(tf.keras.preprocessing.sequence.pad_sequences(ret, maxlen=max_window, padding='pre'))\n",
    "    assert ret.shape == (max_window,205), ret.shape\n",
    "    return ret\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b04d1af2c54f74aa7b73333f13d130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency\n",
      "InstlmentMode\n",
      "LoanStatus\n",
      "PaymentMode\n",
      "BranchID\n",
      "Area\n",
      "ManufacturerID\n",
      "SupplierID\n",
      "SEX\n",
      "City\n",
      "State\n",
      "ZiPCODE\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashutosh/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_label_encoders = {}\n",
    "target_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "\n",
    "train_cat_cols = ['Frequency', 'InstlmentMode', 'LoanStatus', 'PaymentMode', 'BranchID', 'Area', \n",
    "            'ManufacturerID', 'SupplierID', 'SEX', 'City', 'State', 'ZiPCODE']\n",
    "target_col = ['Top-up Month']\n",
    "train_reg_cols = ['AmountFinance', 'DisbursalAmount', 'EMI', 'AssetID', 'MonthlyIncome', 'Tenure', 'AssetCost', 'LTV', 'AGE']\n",
    "train_date_cols = ['DisbursalDate', 'MaturityDAte', 'AuthDate']\n",
    "\n",
    "for col in train_date_cols:\n",
    "    train_df[col] = pd.to_datetime(train_df[col], errors=\"coerce\")\n",
    "    test_df[col] = pd.to_datetime(test_df[col], errors=\"coerce\")\n",
    "    \n",
    "for col in tqdm(train_cat_cols):\n",
    "    if col not in train_label_encoders:\n",
    "        train_label_encoders[col] = LabelEncoder()\n",
    "    print(col)\n",
    "    if train_df[col].dtype == \"float64\":\n",
    "        train_df[col] = train_df[col].fillna(-1.).astype(np.int64)\n",
    "        \n",
    "    if test_df[col].dtype == \"float64\":\n",
    "        test_df[col] = test_df[col].fillna(-1.).astype(np.int64)\n",
    "    fill_val = -1 if train_df[col].dtype == \"int64\" else \"nan\"\n",
    "    if col == target_col[0]:\n",
    "        train_label_encoders[col].fit(train_df[col].fillna(fill_val))\n",
    "\n",
    "    else: train_label_encoders[col].fit(train_df[col].append(test_df[col]).fillna(fill_val))\n",
    "\n",
    "target_encoder.fit(train_df[target_col])\n",
    "\n",
    "def train_encode_cat_cols(x, col, tpe):\n",
    "\n",
    "    if pd.isnull(x): \n",
    "        if tpe == \"object\": x = str(x)\n",
    "        elif tpe == \"int64\": x = -1\n",
    "        else: assert False, x\n",
    "        \n",
    "    return train_label_encoders[col].transform([x])   \n",
    "\n",
    "def encode_reg_cols(x):\n",
    "    if pd.isnull(x):\n",
    "        return -1.\n",
    "    return np.float64(x)\n",
    "\n",
    "\n",
    "def encode_cat_cols(x, col):\n",
    "\n",
    "    if pd.isnull(x): x = str(x)\n",
    "    return np.array(label_encoder_dict[col].transform([x]), dtype=np.float64)\n",
    "\n",
    "\n",
    "def encode_target(x):\n",
    "    return target_encoder.transform(x)\n",
    "\n",
    "\n",
    "def encode_date_cols(x):\n",
    "    if pd.isnull(x):\n",
    "        return [-1, -1, -1, -1, -1]\n",
    "    else:\n",
    "        return np.array([x.hour, x.minute, x.day, x.month, x.year], dtype=np.float64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_max_len = 36\n",
    "def generate_training_data(row):\n",
    "    row = row[0]\n",
    "    ret = []\n",
    "    columns = ['ID', 'Frequency', 'InstlmentMode', 'LoanStatus', 'PaymentMode',\n",
    "       'BranchID', 'Area', 'Tenure', 'AssetCost', 'AmountFinance',\n",
    "       'DisbursalAmount', 'EMI', 'DisbursalDate', 'MaturityDAte', 'AuthDate',\n",
    "       'AssetID', 'ManufacturerID', 'SupplierID', 'LTV', 'SEX', 'AGE',\n",
    "       'MonthlyIncome', 'City', 'State', 'ZiPCODE']\n",
    "    column_tpes = ['int64', 'object', 'object', 'object', 'object', \n",
    "                   'int64', 'object', 'int64', 'int64','float64', \n",
    "                   'float64', 'float64',  '<M8[ns]', '<M8[ns]', '<M8[ns]',\n",
    "                   'int64', 'int64', 'int64', 'float64', 'object', \n",
    "                   'float64', 'float64', 'object', 'object', 'int64', 'object']\n",
    "    \n",
    "    for index in range(len(columns)):\n",
    "        if columns[index] in train_cat_cols:\n",
    "            \n",
    "            ret.extend(train_encode_cat_cols(row[index], columns[index], column_tpes[index]))\n",
    "\n",
    "        elif columns[index] in train_reg_cols:\n",
    "            ret.append(encode_reg_cols(row[index]))\n",
    "\n",
    "        elif columns[index] in train_date_cols:\n",
    "            ret.extend(encode_date_cols(row[index]))\n",
    "        else: pass\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "def generate_datasets_to_train_for_one_user(train_dframe, bureau_df):\n",
    "    X_br = []\n",
    "    X = []\n",
    "    \n",
    "    X_br = encode_df_for_user(bureau_df)\n",
    "    X = generate_training_data(train_dframe.to_numpy())\n",
    "    assert X_br.shape == (max_window, ts_feature_vector), X_br.shape\n",
    "    \n",
    "    assert X.shape == (train_max_len, ), X.shape\n",
    "    return np.reshape(X, (1, X.shape[0])), np.reshape(X_br, (X_br.shape[0], 1, X_br.shape[1]))\n",
    "\n",
    "def generate_datasets_to_train(train_dframe, bureau_df, val_size=.2):\n",
    "    ids = train_dframe[\"ID\"].unique()\n",
    "    np.random.shuffle(ids)\n",
    "    sp = int((1. - val_size) * ids.shape[0])\n",
    "    tr_ids, val_ids = ids[: sp], ids[sp:]\n",
    "    y, y_val = [None] * sp, [None] * int(ids.shape[0] * val_size)\n",
    "    \n",
    "    \n",
    "    X = Parallel(n_jobs=num_cores)(delayed(generate_training_data)(train_dframe[train_dframe.ID == i].to_numpy()) for i in tqdm(tr_ids, total=len(tr_ids)))\n",
    "    X_br = Parallel(n_jobs=num_cores)(delayed(encode_df_for_user)(bureau_df[bureau_df.ID == i]) for i in tqdm(tr_ids, total=len(tr_ids)))\n",
    "    \n",
    "    X_val = Parallel(n_jobs=num_cores)(delayed(generate_training_data)(train_dframe[train_dframe.ID == i].to_numpy()) for i in tqdm(val_ids, total=len(val_ids)))\n",
    "    X_val_br = Parallel(n_jobs=num_cores)(delayed(encode_df_for_user)(bureau_df[bureau_df.ID == i]) for i in tqdm(val_ids, total=len(val_ids)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(len(tr_ids))):\n",
    "        assert train_dframe[train_dframe.ID == tr_ids[i]][target_col].values.shape[0] == 1\n",
    "        y[i] = train_dframe[train_dframe.ID == tr_ids[i]][target_col].values[0]\n",
    "    \n",
    "    y = keras.utils.to_categorical(target_encoder.transform(y))\n",
    "    for i in tqdm(range(len(val_ids))):\n",
    "        assert train_dframe[train_dframe.ID == val_ids[i]][target_col].values.shape[0] == 1\n",
    "        y_val[i] = train_dframe[train_dframe.ID == val_ids[i]][target_col].values[0]\n",
    "    \n",
    "    y_val = keras.utils.to_categorical(target_encoder.transform(y_val))\n",
    "    \n",
    "    return np.array(X), np.array(X_val), np.array(X_br), np.array(X_val_br), np.array(y), np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = generate_datasets_to_train(train_df.sample(10000), df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, list_ids, bs=64, test=False):\n",
    "        self.bs = bs\n",
    "        self.list_ids = list_ids\n",
    "        self.n_classes = 7\n",
    "        self.shuffle = False if test else True\n",
    "        self.test = test\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.list_ids) / self.bs))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        idx_min = index * self.bs\n",
    "        idx_max = min(idx_min + self.bs, len(self.list_ids))\n",
    "        indexes = self.indexes[idx_min: idx_max]\n",
    "        \n",
    "        temp_list_ids = [self.list_ids[k] for k in indexes]\n",
    "        if self.test:\n",
    "            X = self.__data_generator(temp_list_ids)\n",
    "            return X\n",
    "        else:\n",
    "            X, y = self.__data_generator(temp_list_ids)\n",
    "            return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_ids))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __data_generator(self, temp_list):\n",
    "        X = [None] * len(temp_list)\n",
    "        y = [None] * len(temp_list)\n",
    "        for index in range(len(temp_list)):\n",
    "            i = temp_list[index]\n",
    "            if self.test:\n",
    "                X[index] = generate_datasets_to_train_for_one_user(\n",
    "                    test_df[test_df.ID == i],\n",
    "                    tdf[tdf.ID == i]\n",
    "                )\n",
    "            else:\n",
    "                X[index] = generate_datasets_to_train_for_one_user(\n",
    "                    train_df[train_df.ID == i],\n",
    "                    df[df.ID == i]\n",
    "                )\n",
    "                \n",
    "                y[index] = train_df[train_df.ID == i][target_col].values[0]\n",
    "                \n",
    "        if self.test:\n",
    "            return X\n",
    "#         print(X)\n",
    "        y = target_encoder.transform(y)\n",
    "        y = keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = .2\n",
    "ids = train_df[\"ID\"].unique()\n",
    "np.random.shuffle(ids)\n",
    "sp = int((1. - val_size) * ids.shape[0])\n",
    "tr_ids, val_ids = ids[: sp], ids[sp:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(tr_ids)\n",
    "val_gen = DataGenerator(val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets_to_test(train_dframe, bureau_df):\n",
    "    tr_ids = train_dframe[\"ID\"].unique()\n",
    "    \n",
    "    X = Parallel(n_jobs=num_cores)(delayed(generate_training_data)(train_dframe[train_dframe.ID == i].to_numpy()) for i in tqdm(tr_ids, total=len(tr_ids)))\n",
    "    X_br = Parallel(n_jobs=num_cores)(delayed(encode_df_for_user)(bureau_df[bureau_df.ID == i]) for i in tqdm(tr_ids, total=len(tr_ids)))\n",
    "    \n",
    "    return np.array(X), np.array(X_br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 420, 205)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 420, 128)     171008      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 36)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 420, 64)      49408       lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 36)           1332        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 420, 36)      14544       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 36)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 15120)        0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 15156)        0           dropout[0][0]                    \n",
      "                                                                 flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 15156)        0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           485024      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7)            231         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 721,547\n",
      "Trainable params: 721,547\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    train_in = Input(shape=(train_max_len, ))\n",
    "    train_int = Dense(36,activation=\"relu\")(train_in)\n",
    "    train_int = Dropout(.2, seed=42)(train_int)\n",
    "    bureau_in = Input(shape=(max_window, ts_feature_vector))\n",
    "    bureau_int = LSTM(128, kernel_initializer='he_uniform', return_sequences=True)(bureau_in)\n",
    "    bureau_int = LSTM(64, kernel_initializer='he_uniform', return_sequences=True)(bureau_int)\n",
    "    bureau_int = LSTM(36, kernel_initializer='he_uniform', return_sequences=True)(bureau_int)\n",
    "#     bureau_int = Reshape((420, 36,))(bureau_int)\n",
    "#     bureau_int = Dense(32,activation=\"relu\")(bureau_int)\n",
    "    bureau_int = Flatten()(bureau_int)\n",
    "    x = Concatenate()([train_int, bureau_int])\n",
    "#     x = Merge([train_int, bureau_int], mode='concat')\n",
    "    x = Dropout(.2, seed=42)(x)\n",
    "    \n",
    "    x = Dense(32,activation=\"relu\")(x)\n",
    "    output = Dense(7, activation=\"softmax\")(x)\n",
    "    model = Model([train_in, bureau_in], output)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=f1_loss,\n",
    "        metrics=[\"acc\", f1_loss],\n",
    "    )\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "model = get_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'model_save/model.keras'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=f1_loss, patience=3, verbose=1,\n",
    "    mode=\"min\", restore_best_weights=True\n",
    ")\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=f1_loss, factor=0.1, patience=3, verbose=1,\n",
    "    mode='min', min_delta=0.0001, cooldown=0, min_lr=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1608/1608 [==============================] - ETA: 0s - loss: 0.9917 - acc: 0.0320 - f1_loss: 0.9917\n",
      "Epoch 00001: saving model to model_save/model.keras\n",
      "1608/1608 [==============================] - 305s 190ms/step - loss: 0.9917 - acc: 0.0320 - f1_loss: 0.9917 - val_loss: 0.9921 - val_acc: 0.0289 - val_f1_loss: 0.9921 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f07622b0e90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(x=[z[0], z[2]], y=z[4], validation_data=([z[1], z[3]], z[5]), \n",
    "#           epochs=2, batch_size=32, shuffle=True)\n",
    "model.fit_generator(generator=train_gen, validation_data=val_gen, \n",
    "                    epochs=1,\n",
    "                   use_multiprocessing=True, workers=7, callbacks=[cp_callback, early_stopping, plateau])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = generate_datasets_to_test(test_df.sample(1000), tdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_ids = test_df[\"ID\"]\n",
    "\n",
    "test_gen = DataGenerator(test_ids, bs= 1, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14745/14745 [==============================] - 95s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_generator(test_gen, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14745, 7)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14745, 25)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({\"ID\": test_df[\"ID\"], target_col[0]: target_encoder.inverse_transform(np.argmax(predictions, axis=1))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'24-30 Months': 14745}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = {}\n",
    "for i in target_encoder.inverse_transform(np.argmax(predictions, axis=1)):\n",
    "    if i not in count:\n",
    "        count[i] = 0\n",
    "    count[i] += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
