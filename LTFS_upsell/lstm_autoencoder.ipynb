{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow import keras\n",
    "import os, logging\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import pickle as pkl\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Embedding, Bidirectional, Dropout, concatenate, SpatialDropout1D, GlobalMaxPooling1D, Reshape, MaxPooling1D, Flatten, Conv1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "df = pd.read_csv(\"Train/cleaned_bureau.csv\")\n",
    "tdf = pd.read_csv(\"Test/cleaned_bureau.csv\")\n",
    "train_df = pd.read_csv(\"Train/cleaned_train.csv\")\n",
    "test_df = pd.read_csv(\"Test/cleaned_train.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ID.value_counts(sort=True, ascending=False), tdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.ID.value_counts(sort=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['SELF-INDICATOR', 'MATCH-TYPE', 'ACCT-TYPE', 'CONTRIBUTOR-TYPE',\n",
    "       'OWNERSHIP-IND', 'ACCOUNT-STATUS', 'INSTALLMENT-TYPE',\n",
    "       'ASSET_CLASS', 'INSTALLMENT-FREQUENCY',\n",
    "       'DPD - HIST']    #should not be here\n",
    "date_cols = ['DATE-REPORTED',  'DISBURSED-DT', 'CLOSE-DT', \n",
    "             'LAST-PAYMENT-DATE']\n",
    "reg_cols = ['CREDIT-LIMIT/SANC AMT', 'DISBURSED-AMT/HIGH CREDIT', 'INSTALLMENT-AMT', 'CURRENT-BAL',\n",
    "        'OVERDUE-AMT', 'WRITE-OFF-AMT', 'TENURE'] # , 'DPD - HIST']\n",
    "array_cols = ['REPORTED DATE - HIST', 'CUR BAL - HIST',\n",
    "       'AMT OVERDUE - HIST', 'AMT PAID - HIST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cat_cols) + len(date_cols) + len(reg_cols) + len(array_cols), df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([df[i].dtype in (\"bool\" ,\"object\") for i in cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([df[i].dtype == \"float64\" for i in reg_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[date_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in date_cols:\n",
    "    df[col] = pd.to_datetime(df[col])\n",
    "    tdf[col] = pd.to_datetime(tdf[col])\n",
    "df[date_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length(amt):    \n",
    "    if not pd.isnull(amt):\n",
    "        return len(amt.split(\",\"))\n",
    "    else: 0\n",
    "        \n",
    "for col in array_cols:\n",
    "    print(\"max train {}\".format(df[col].apply(lambda x: get_length(x)).max()))\n",
    "    \n",
    "    print(\"max test {}\".format(tdf[col].apply(lambda x: get_length(x)).max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_window = 420\n",
    "ts_feature_vector = 205\n",
    "max_array_size = 42\n",
    "label_encoder_dict = {}\n",
    "num_cores = 7\n",
    "\n",
    "def encode_reg_cols(x):\n",
    "    if pd.isnull(x):\n",
    "        return 0.\n",
    "    return np.float64(x)\n",
    "\n",
    "def encode_array_cols(x):\n",
    "    ret = [0] * max_array_size\n",
    "    if not pd.isnull(x):\n",
    "        l = x.split(\",\")\n",
    "        y = max_array_size - len(l)\n",
    "        for index in range(len(l)):\n",
    "            try:\n",
    "                ret[y + index] = np.float32(l[index])\n",
    "            except:\n",
    "                ret[y + index] = 0.\n",
    "    return ret\n",
    "\n",
    "\n",
    "def encode_date_cols(x):\n",
    "    if pd.isnull(x):\n",
    "        return [-1., -1., -1., -1., -1.]\n",
    "    else:\n",
    "        return np.array([x.hour, x.minute, x.day, x.month, x.year], dtype=np.float64)\n",
    "\n",
    "\n",
    "def encode_cat_cols(x, col):\n",
    "\n",
    "    if pd.isnull(x): x = str(x)\n",
    "    return np.array(label_encoder_dict[col].transform([x]), dtype=np.float64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for col in tqdm(cat_cols):\n",
    "    if col not in label_encoder_dict:\n",
    "        label_encoder_dict[col] = LabelEncoder()\n",
    "    print(col)\n",
    "    label_encoder_dict[col].fit(df[col].append(tdf[col]).fillna(\"nan\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df = df[df.ID == 141732]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_df_for_user(dframe):\n",
    "    \n",
    "    final = []\n",
    "    for index, row in dframe.iterrows():\n",
    "        \n",
    "        l = [None] * 4\n",
    "        ret = np.array([], dtype=np.float64)\n",
    "    \n",
    "        # 10 * 1\n",
    "        for col in cat_cols:\n",
    "            ret = np.concatenate((ret, encode_cat_cols(row[col], col)))\n",
    "        \n",
    "        # 7 * 1 = 7\n",
    "        for col in reg_cols:\n",
    "            ret = np.concatenate((ret, np.array([encode_reg_cols(row[col])])))\n",
    "        \n",
    "        # 5 * 4 = 20\n",
    "        for col in date_cols:\n",
    "            ret = np.concatenate((ret, encode_date_cols(row[col])))\n",
    "        \n",
    "        # 4 * 42 = 168\n",
    "        for i in range(4):\n",
    "            l[i] = encode_array_cols(array_cols[i])\n",
    "            \n",
    "        \n",
    "        ret = np.concatenate((ret, np.array(tf.keras.preprocessing.sequence.pad_sequences(l, maxlen=max_array_size, padding='pre')).flatten()))\n",
    "        assert len(ret) == ts_feature_vector, print(len(l), len(l[0]))\n",
    "        final.append(ret)\n",
    "    while len(final) < max_window:\n",
    "        final.insert(0, [0.] * ts_feature_vector)\n",
    "    return np.array(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_df_for_user(temp_df).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_encoders = {}\n",
    "target_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "\n",
    "train_cat_cols = ['Frequency', 'InstlmentMode', 'LoanStatus', 'PaymentMode', 'BranchID', 'Area', \n",
    "            'ManufacturerID', 'SupplierID', 'SEX', 'City', 'State', 'ZiPCODE']\n",
    "target_col = ['Top-up Month']\n",
    "train_reg_cols = ['AmountFinance', 'DisbursalAmount', 'EMI', 'AssetID', 'MonthlyIncome', 'Tenure', 'AssetCost', 'LTV', 'AGE']\n",
    "train_date_cols = ['DisbursalDate', 'MaturityDAte', 'AuthDate']\n",
    "\n",
    "for col in train_date_cols:\n",
    "    train_df[col] = pd.to_datetime(train_df[col], errors=\"coerce\")\n",
    "    test_df[col] = pd.to_datetime(test_df[col], errors=\"coerce\")\n",
    "    \n",
    "for col in tqdm(train_cat_cols):\n",
    "    if col not in train_label_encoders:\n",
    "        train_label_encoders[col] = LabelEncoder()\n",
    "    print(col)\n",
    "    fill_val = -1 if train_df[col].dtype == \"int64\" else \"nan\"\n",
    "    if col == target_col[0]:\n",
    "        train_label_encoders[col].fit(train_df[col].fillna(fill_val))\n",
    "\n",
    "    else: train_label_encoders[col].fit(train_df[col].append(test_df[col]).fillna(fill_val))\n",
    "\n",
    "target_encoder.fit(train_df[target_col])\n",
    "\n",
    "def train_encode_cat_cols(x, col, tpe):\n",
    "\n",
    "    if pd.isnull(x): \n",
    "        if tpe == \"object\": x = str(x)\n",
    "        elif x == \"int64\": x = 0\n",
    "        else: assert False\n",
    "        \n",
    "    return train_label_encoders[col].transform([x])   \n",
    "\n",
    "def encode_target(x):\n",
    "    return target_encoder.transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_max_len = 36\n",
    "def generate_training_data(row):\n",
    "    row = row[0]\n",
    "    ret = []\n",
    "    columns = ['ID', 'Frequency', 'InstlmentMode', 'LoanStatus', 'PaymentMode',\n",
    "       'BranchID', 'Area', 'Tenure', 'AssetCost', 'AmountFinance',\n",
    "       'DisbursalAmount', 'EMI', 'DisbursalDate', 'MaturityDAte', 'AuthDate',\n",
    "       'AssetID', 'ManufacturerID', 'SupplierID', 'LTV', 'SEX', 'AGE',\n",
    "       'MonthlyIncome', 'City', 'State', 'ZiPCODE']\n",
    "    column_tpes = ['int64', 'object', 'object', 'object', 'object', \n",
    "                   'int64', 'object', 'int64', 'int64','float64', \n",
    "                   'float64', 'float64',  '<M8[ns]', '<M8[ns]', '<M8[ns]',\n",
    "                   'int64', 'int64', 'int64', 'float64', 'object', \n",
    "                   'float64', 'float64', 'object', 'object', 'int64', 'object']\n",
    "    \n",
    "    for index in range(len(columns)):\n",
    "        if columns[index] in train_cat_cols:\n",
    "            \n",
    "            ret.extend(train_encode_cat_cols(row[index], columns[index], column_tpes[index]))\n",
    "\n",
    "        elif columns[index] in train_reg_cols:\n",
    "            ret.append(encode_reg_cols(row[index]))\n",
    "\n",
    "        elif columns[index] in train_date_cols:\n",
    "            ret.extend(encode_date_cols(row[index]))\n",
    "        else: pass\n",
    "    return np.array(ret)\n",
    "    \n",
    "def generate_datasets_to_train(train_dframe, bureau_df, val_size=.2):\n",
    "    ids = train_dframe[\"ID\"].unique()\n",
    "    np.random.shuffle(ids)\n",
    "    sp = int((1. - val_size) * ids.shape[0])\n",
    "    tr_ids, val_ids = ids[: sp], ids[sp:]\n",
    "    y, y_val = [], []\n",
    "    \n",
    "    \n",
    "    X = Parallel(n_jobs=num_cores)(delayed(generate_training_data)(train_dframe[train_dframe.ID == i].to_numpy()) for i in tqdm(tr_ids, total=len(tr_ids)))\n",
    "    X_br = Parallel(n_jobs=num_cores)(delayed(encode_df_for_user)(bureau_df[bureau_df.ID == i]) for i in tqdm(tr_ids, total=len(tr_ids)))\n",
    "    \n",
    "    X_val = Parallel(n_jobs=num_cores)(delayed(generate_training_data)(train_dframe[train_dframe.ID == i].to_numpy()) for i in tqdm(val_ids, total=len(val_ids)))\n",
    "    X_val_br = Parallel(n_jobs=num_cores)(delayed(encode_df_for_user)(bureau_df[bureau_df.ID == i]) for i in tqdm(val_ids, total=len(val_ids)))\n",
    "    \n",
    "\n",
    "    for i in tqdm(tr_ids):\n",
    "        y.append(target_encoder.transform(train_dframe[train_dframe.ID == i][target_col].values))\n",
    "        \n",
    "    for i in tqdm(val_ids):\n",
    "        y_val.append(target_encoder.transform(train_dframe[train_dframe.ID == i][target_col].values))\n",
    "    return np.array(X), np.array(X_val), np.array(X_br), np.array(X_val_br), np.array(y), np.array(y_val)\n",
    "    \n",
    "\n",
    "def generate_datasets_to_train_for_one_user(train_dframe, bureau_df):\n",
    "    \n",
    "    X_br = []\n",
    "    X = []\n",
    "    \n",
    "    X_br.append(encode_df_for_user(bureau_df))\n",
    "    X.append(generate_training_data(train_dframe.to_numpy()))\n",
    "    return np.array(X), np.array(X_br)\n",
    "    \n",
    "def generate_datasets_to_train_for_one_user_test(train_dframe, bureau_df):\n",
    "    \n",
    "    X_br =  []\n",
    "    X = []\n",
    "    X_br.append(encode_df_for_user(bureau_df))\n",
    "    X.append(generate_training_data(train_dframe.to_numpy()))\n",
    "    return np.array(X), np.array(X_br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, list_ids, bs=batch_size, test=False):\n",
    "        self.bs = bs\n",
    "        self.list_ids = list_ids\n",
    "        self.n_classes = 7\n",
    "        self.shuffle = False if test else True\n",
    "        self.test = test\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.list_ids) / self.bs))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        idx_min = index * self.bs\n",
    "        idx_max = min(idx_min + self.bs, len(self.list_ids))\n",
    "        indexes = self.indexes[idx_min: idx_max]\n",
    "        \n",
    "        temp_list_ids = [self.list_ids[k] for k in indexes]\n",
    "        if self.test:\n",
    "            X = self.__data_generator(temp_list_ids)\n",
    "            return X\n",
    "        else:\n",
    "            X, y = self.__data_generator(temp_list_ids)\n",
    "            return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_ids))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __data_generator(self, temp_list):\n",
    "        X = [None] * len(temp_list)\n",
    "        y = [None] * len(temp_list)\n",
    "        for index in range(len(temp_list)):\n",
    "            i = temp_list[index]\n",
    "            if self.test:\n",
    "                X[index] = generate_datasets_to_train_for_one_user_test(\n",
    "                    test_df[test_df.ID == i],\n",
    "                    tdf[tdf.ID == i]\n",
    "                )\n",
    "            else:\n",
    "                X[index] = generate_datasets_to_train_for_one_user(\n",
    "                    train_df[train_df.ID == i],\n",
    "                    df[df.ID == i]\n",
    "                )\n",
    "                \n",
    "                y[index] = train_df[train_df.ID == i][target_col].values[0]\n",
    "                \n",
    "        if self.test:\n",
    "            return X\n",
    "\n",
    "        y = target_encoder.transform(y)\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    train_in = Input(shape=(train_max_len, ))\n",
    "    train_int = Dense(32,activation=\"relu\")(train_in)\n",
    "    train_int = Dropout(.2, seed=42)(train_int)\n",
    "    bureau_in = Input(shape=(max_window, ts_feature_vector))\n",
    "    bureau_int = LSTM(128, kernel_initializer='he_uniform', return_sequences=True)(bureau_in)\n",
    "    bureau_int = LSTM(64, kernel_initializer='he_uniform', return_sequences=True)(bureau_int)\n",
    "    bureau_int = LSTM(36, kernel_initializer='he_uniform', return_sequences=True)(bureau_int)\n",
    "    bureau_int = Reshape((420*36,), input_shape=(None, 420, 36))(bureau_int)\n",
    "    x = concatenate([train_int, bureau_int])\n",
    "    x = Dropout(.2, seed=42)(x)\n",
    "    \n",
    "    x = Dense(32,activation=\"relu\")(x)\n",
    "    output = Dense(7, activation=\"softmax\")(x)\n",
    "    model = Model([train_in, bureau_in], output)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=f1_loss,\n",
    "        metrics=[\"acc\", f1_loss],\n",
    "    )\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = .2\n",
    "ids = train_df[\"ID\"].unique()\n",
    "np.random.shuffle(ids)\n",
    "sp = int((1. - val_size) * ids.shape[0])\n",
    "tr_ids, val_ids = ids[: sp], ids[sp:]\n",
    "test_ids = test_df[\"ID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(tr_ids, bs=batch_size)\n",
    "val_gen = DataGenerator(val_ids)\n",
    "test_gen = DataGenerator(test_ids, test=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'model_save/model.keras'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=f1_loss, patience=3, verbose=1,\n",
    "    mode=\"min\", restore_best_weights=True\n",
    ")\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=f1_loss, factor=0.1, patience=3, verbose=1,\n",
    "    mode='min', min_delta=0.0001, cooldown=0, min_lr=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator=train_gen, validation_data=val_gen, \n",
    "                    epochs=20,\n",
    "                   use_multiprocessing=True, workers=7, callbacks=[cp_callback, early_stopping, plateau])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('model_save/model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(test_gen, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('model_save/model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
